{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦  loading timm/miniâ€‘imagenet â€¦\n",
      "âœ“ dataset ready â€” 8192 images\n",
      "\n",
      "ðŸš€  processing mixer_b16_224 â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d682c90b1164612bbe0a8cfdee6319c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/240M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9927da75cb0c4052a33bad950acddbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "           mixer_b16_224:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with looping\n",
      "features computed\n",
      "   â†³ saved  kernels_out_mi_no_pool/K_mixer_b16_224_8192.pt\n",
      "\n",
      "ðŸš€  processing gcvit_base â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_base_224_nvidia-f009139b.pth\" to /home/user/.cache/torch/hub/checkpoints/gcvit_base_224_nvidia-f009139b.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c579df08e84fbeb62daa6ccac5ef92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "              gcvit_base:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with looping\n",
      "features computed\n",
      "   â†³ saved  kernels_out_mi_no_pool/K_gcvit_base_8192.pt\n",
      "\n",
      "ðŸš€  processing convnextv2_base â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428d69a91d7047c88704d1bba6bb321b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/355M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71aa3d3fcea4ac68c87e7923b9af7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "         convnextv2_base:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with looping\n",
      "features computed\n",
      "   â†³ saved  kernels_out_mi_no_pool/K_convnextv2_base_8192.pt\n",
      "\n",
      "ðŸš€  processing clip_vit_base_patch32 â€¦\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unknown model (clip_vit_base_patch32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 143\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MODEL_NAMES:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸš€  processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m â€¦\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m     F_m \u001b[38;5;241m=\u001b[39m features(m)                            \u001b[38;5;66;03m# (N, D_m)\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures computed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    145\u001b[0m     Z_m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(F_m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)           \u001b[38;5;66;03m# rowâ€‘norm\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[4], line 125\u001b[0m, in \u001b[0;36mfeatures\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeatures\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 125\u001b[0m     model \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(model_name, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m                               num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    127\u001b[0m     vecs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imgs, _ \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>24\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# Process in smaller batches if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/timm/models/_factory.py:122\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, cache_dir, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m         pretrained_cfg \u001b[38;5;241m=\u001b[39m pretrained_tag\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_model(model_name):\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown model (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m model_name)\n\u001b[1;32m    124\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unknown model (clip_vit_base_patch32)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "mini_imagenet_kernels_per_model.py\n",
    "----------------------------------\n",
    "â€¢ Uses the open timm/miniâ€‘imagenet dataset (50â€¯k images, 100 classes)\n",
    "â€¢ For each timm encoder in MODEL_NAMES:\n",
    "      â€“ extracts features on N_IMAGES random samples\n",
    "      â€“ builds its own cosineâ€‘similarity kernel  K = Z Záµ€\n",
    "      â€“ saves to  kernels_out/K_<model>.pt\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ userâ€‘tweakables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "N_IMAGES   = 8_192\n",
    "BATCH_SIZE = 1024\n",
    "NUM_WORKERS = 0\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    # â”€â”€ Classic CNNs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    #\"resnet18\",\n",
    "    #\"resnet34\",\n",
    "    #\"resnet50\",\n",
    "    #\"resnet101\",\n",
    "    #\"resnet152\",\n",
    "    #\"wide_resnet50_2\",\n",
    "    #\"wide_resnet101_2\",\n",
    "    #\"resnext50_32x4d\",\n",
    "    #\"resnext101_32x8d\",\n",
    "    #\"densenet121\",\n",
    "    #\"densenet201\",\n",
    "    #\"ese_vovnet39b\",\n",
    "    #\"regnety_016\",\n",
    "    #\"regnety_032\",\n",
    "    # ConvNeXt family\n",
    "    #\"convnext_small\",\n",
    "    # EfficientNet & friends\n",
    "    #\"efficientnet_b0\",\n",
    "    # Mobile / lightweight\n",
    "    #\"mobilenetv3_large_100\",\n",
    "    #\"ghostnet_100\",\n",
    "    # NFâ€‘Nets (DeepMind)\n",
    "    #\"dm_nfnet_f0\",\n",
    "    # â”€â”€ Vision Transformers & hybrids â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # ViT\n",
    "    #\"vit_base_patch16_224\",\n",
    "    # DeiT\n",
    "    #\"deit_tiny_patch16_224\",\n",
    "    #\"deit_small_patch16_224\",\n",
    "    # BEiT\n",
    "    #\"beit_base_patch16_224\",\n",
    "    #\"beit_large_patch16_224\",\n",
    "    # Swin\n",
    "    #\"swin_tiny_patch4_window7_224\",\n",
    "    # PVTâ€‘v2\n",
    "    #\"pvt_v2_b2\",\n",
    "    # CSWin\n",
    "    #\"cswin_tiny_224\",\n",
    "    # CoAtNet\n",
    "    #\"coatnet_0\",\n",
    "    # Mixers / Convmixer\n",
    "    \"mixer_b16_224\",\n",
    "    # GC ViT\n",
    "    \"gcvit_base\",\n",
    "    # ConvNeXtâ€‘v2\n",
    "    \"convnextv2_base\",\n",
    "    # CLIP ViT (image branch only)\n",
    "    \"clip_vit_base_patch32\",\n",
    "]\n",
    "\n",
    "OUT_DIR = Path(\"kernels_out_mi_no_pool\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "\n",
    "# 1) Dataset -------------------------------------------------------------------\n",
    "print(\"ðŸ“¦  loading timm/miniâ€‘imagenet â€¦\")\n",
    "hf_ds = load_dataset(\"timm/mini-imagenet\", split=\"train\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),   # ensure 3â€‘ch\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class HFWrapper(Dataset):\n",
    "    def __init__(self, ds, tfm):\n",
    "        self.ds, self.tfm = ds, tfm\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self, i):\n",
    "        item = self.ds[int(i)]\n",
    "        return self.tfm(item[\"image\"]), item[\"label\"]\n",
    "\n",
    "# sample once, reuse for every model\n",
    "full_ds   = HFWrapper(hf_ds, transform)\n",
    "indices   = random.sample(range(len(full_ds)), N_IMAGES)\n",
    "subset_ds = Subset(full_ds, indices)\n",
    "loader    = DataLoader(subset_ds, batch_size=BATCH_SIZE,\n",
    "                       shuffle=False, num_workers=NUM_WORKERS,\n",
    "                       pin_memory=True)\n",
    "print(f\"âœ“ dataset ready â€” {len(subset_ds)} images\\n\")\n",
    "\n",
    "\n",
    "# 2) Feature â†’ kernel â†’ save  (one loop per model) -----------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def features(model_name: str) -> torch.Tensor:\n",
    "    model = timm.create_model(model_name, pretrained=True,\n",
    "                              num_classes=0).to(DEVICE).eval()\n",
    "    vecs = []\n",
    "    for imgs, _ in tqdm(loader, desc=f\"{model_name:>24}\", leave=False):\n",
    "        # Process in smaller batches if needed\n",
    "        batch_output = model(imgs.to(DEVICE, non_blocking=True)).flatten(1)\n",
    "        # Convert to float32 for better memory efficiency\n",
    "        batch_output = batch_output.to(torch.float32)\n",
    "        vecs.append(batch_output)\n",
    "        # Explicitly free memory\n",
    "        torch.cuda.empty_cache()\n",
    "    print('done with looping')\n",
    "\n",
    "    return torch.cat(vecs)\n",
    "\n",
    "\n",
    "for m in MODEL_NAMES:\n",
    "    print(f\"ðŸš€  processing {m} â€¦\")\n",
    "    F_m = features(m)                            # (N, D_m)\n",
    "    print('features computed')\n",
    "    Z_m = F.normalize(F_m, p=2, dim=1)           # rowâ€‘norm\n",
    "    K_m = Z_m @ Z_m.T                            # (N, N)\n",
    "\n",
    "    torch.save(\n",
    "        {\"K\": K_m.cpu(),                         # kernel\n",
    "         \"Z\": F_m.cpu(),                         # normalised feats\n",
    "         \"dim\": F_m.shape[1],                    # feature length of this model\n",
    "         \"indices\": indices},\n",
    "        OUT_DIR / f\"K_{m}_{N_IMAGES}.pt\"\n",
    "    )\n",
    "    print(f\"   â†³ saved  {OUT_DIR / f'K_{m}_{N_IMAGES}.pt'}\\n\")\n",
    "\n",
    "    # Clean up memory before next model\n",
    "    del F_m, Z_m, K_m\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()  # Force garbage collection\n",
    "\n",
    "print(\"âœ…  all kernels done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
